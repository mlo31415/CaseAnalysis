import os
import collections
import xml.etree.ElementTree
import WikidotHelpers

#TODO: Need to deal with accented letter (e.g. Farmer)
#TODO: Need to deal with embedded hyperlinks (e.g., Ansible)
#TODO: Need to deal with ALL-CAPS (are we ignoring all of the pages we ought to be?)

log = open("log.txt", "w")

def logger(message):
    print(message, file=log)
    print(message)



# *****************************************************************
# Should this filename be ignored?
# This routine is highly wiki-dependent
# Return value is either the cleaned filename or None if the file should be ignored.
def InterestingFilenameZip(filenameZip):

    if not filenameZip.startswith("source/"):    # We're only interested in source files
        return None
    if len(filenameZip) <= 11:  # There needs to be something there besides just 'source/.txt'
           return None

    # These files are specific to Fancyclopedia and are known to be ignorable
    if filenameZip.startswith("source/deleted_"):   # Ignore deleted pages
        return None
    if filenameZip.startswith("source/nav_"):   # Ignore navigation pages
        return None
    if filenameZip.startswith("source/forum_"):   # Ignore forum pages
        return None
    if filenameZip.startswith("source/testing_"):   # These are test pages of various sorts
        return None
    if filenameZip.startswith("source/system_"):   # Ignore system pages
        return None
    if filenameZip.startswith("source/admin_"):   # Ignore system admin pages
        return None
    if filenameZip.startswith("source/search_"):   # Ignore system search pages
        return None
    if filenameZip.startswith("source/index_"):   # Ignore our index pages
        return None
    if filenameZip.startswith("source/most-wanted-pages"): # Ignore the *previous* most-wanted-pages page
        return None

    return filenameZip[7:-4]  # Drop "source/" and ".txt", returning the cleaned name

PageInfo=collections.namedtuple("PageInfo", "Title, CanName, Tags, Links, Redirect")

#==================================================================
# Load the pages in the site directory
def LoadDirectory(site, dir):
    if not os.path.isdir(dir):
        return

    # First deal with any pages in this directory
    for (dirpath, dirnames, filenames) in os.walk(dir):
        break

    filenames=[os.path.splitext(f)[0] for f in filenames if os.path.splitext(f)[1].lower() == ".txt"]
    for fname in filenames:
        LoadPage(site, dirpath, fname)

# Load a single page
def LoadPage(site, dirpath, fname):

    pathname=os.path.join(dirpath, fname)

    # Read the tags and title from fname.xml
    e=xml.etree.ElementTree.parse(pathname+".xml").getroot()
    title=e.find("title").text

    tagsEl=e.find("tags")
    tags=None
    if tagsEl is not None:
        tags=[t.text for t in tagsEl.findall("tag")]

    #print(fname)

    f=open(os.path.join(pathname+".txt"), "rb") # Reading in binary and doing the funny decode is to handle special characters embedded in some sources.
    source=f.read().decode("cp437")
    f.close

    # First, check to see if this is a redirect.  If it is, we're done.
    redirect=WikidotHelpers.IsRedirect(source)
    if redirect:
        site[fname]=PageInfo(title, fname, tags, None, redirect)
        return

    # Now we scan the source for links.
    # A link is one of these formats:
    #   [[[link]]]
    #   [[[link|display text]]]

    links=[]
    while len(source) > 0:
        loc=source.find("[[[")
        if loc == -1:
            break
        loc2=source.find("]]]", loc)
        if loc2 == -1:
            break
        link=source[loc+3:loc2]
        links.append(link)
        source=source[loc2:]

    site[fname]=PageInfo(title, fname, tags, links, None)

    return

# *****************************************************************
# *****************************************************************
# Main

root=r"C:\Users\mlo\Documents\usr\Fancyclopedia\Python\site"

# Walk the directory structure under root
# We want the following information for each existing page:
#   Its title
#   Its cannonical name
#   Its tags
#   Its Links (a list of the exact link text for each link)
#   If it is a redirect, the exact text of the redirect page name
# This will be stored as a dictionary indexed by the page's cannonical name
# The value will be a tuple: (Title, CanName, Tags, list of Links, Redirect)

# Note that each page has *four* components in the site image on disk:
#   <name>.txt -- the source text
#   <name>.xml -- xml containing (among other things) the tags
#   <name>.html> -- the html generated by Wikidot from the source
#   <name> as a directory -- if there are attached files, a directory named <name> containing the files

site={}
LoadDirectory(site, root)

# Now we have a complete map of the links in site.
# The map lists the links *leaving* each page.  We want to generate the inverted map showing the links *in* to each page.

def AddLink(inverseSite, link, name):
    if link in inverseSite:
        inverseSite[link].append(name)
    else:
        inverseSite[link]=[name]

inverseSite={}      # A dictionary of all pages (existing or not) with a list of the pages that point to it. This is case-sensitive.
for (key, val) in site.items():
    if val.Links is not None:
        for link in val.Links:
            AddLink(inverseSite, link, key)
    else:
        if val.Redirect is not None:
            AddLink(inverseSite, val.Redirect, key)

i=0




